{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7570e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x01_populationStatistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12887f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chewc.callback import *\n",
    "from chewc.policy import *\n",
    "from chewc.sim import *\n",
    "from chewc.lab import *\n",
    "from chewc.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9e0107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "import pdb\n",
    "import torch\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "device='cpu'\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be863803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import numpy as np\n",
    "from chewc.sim import *\n",
    "import torch\n",
    "\n",
    "class SelectionIntensityEnvironment(gym.Env):\n",
    "    def __init__(self, SP, config):\n",
    "        super(SelectionIntensityEnvironment, self).__init__()\n",
    "        self.SP = SP\n",
    "        self.config = config  # Store the config\n",
    "        self.current_generation = 0\n",
    "        self.max_generations = SP.max_generations\n",
    "        # Get action space bounds from config, with defaults if not provided\n",
    "        \n",
    "        self.action_low = config.get('action_low', 0.05)\n",
    "        self.action_high = config.get('action_high', 0.95)\n",
    "        \n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=np.array([self.action_low]), \n",
    "            high=np.array([self.action_high]), \n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Update observation space based on config\n",
    "#         obs_config = config['observation_config']['remaining_proportion']\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=np.array([0, 0]),  # Lower bounds for remaining proportion and genetic variance\n",
    "            high=np.array([1, np.inf]),  # Upper bounds (genetic variance can be any positive number)\n",
    "            shape=(2,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        # logging\n",
    "        self.action_values = []\n",
    "        self.genetic_variance = []\n",
    "        self.max_breeding_values = []\n",
    "        self.final_generations = []\n",
    "        self.episode_count = 0\n",
    "        self.rewards = []\n",
    "        self.episode_reward = 0\n",
    "        \n",
    "        #config\n",
    "        self.config =config\n",
    "        \n",
    "    def _get_obs(self):\n",
    "        remaining_proportion = 1 - (self.current_generation / self.max_generations)\n",
    "        genetic_variance = self.population.breeding_values.var().cpu().item()\n",
    "        return np.array([remaining_proportion, genetic_variance], dtype=np.float32)\n",
    "\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"max_phenotype\": self.population.breeding_values.max().cpu().item(),\n",
    "            \"genetic_variance\": self.population.breeding_values.var().cpu().item(),\n",
    "            \"current_generation\": self.current_generation\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.population = self.SP.founder_pop\n",
    "        self.phenotype = phenotype(self.population, self.SP.T, self.SP.h2)\n",
    "        self.current_generation = 0\n",
    "        self.episode_reward = 0\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        \n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        # Map the action from [-1, 1] to [action_low, action_high]\n",
    "        action = scale_values(action, to_range=(self.action_low, self.action_high))\n",
    "\n",
    "        total_selected = max((2,int(action * self.population.size)))\n",
    "        selected = torch.topk(self.population.phenotypes, total_selected).indices\n",
    "        self.population = create_pop(self.SP.G, random_crosses(self.population.haplotypes[selected], self.SP.pop_size))\n",
    "        self.phenotype = phenotype(self.population, self.SP.T, self.SP.h2)\n",
    "        self.current_generation += 1\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        info['normalized_action'] = action\n",
    "        \n",
    "        terminated = self.current_generation > self.SP.max_generations\n",
    "        #REWARD\n",
    "        if self.config.get('sparse_reward', False):  # Use .get() with a default value\n",
    "            reward = 0 if not terminated else float(self.population.breeding_values.max())\n",
    "        else:\n",
    "            reward = float(self.population.breeding_values.max())\n",
    "        self.episode_reward += reward\n",
    "\n",
    "        if terminated:\n",
    "            info['final_generation'] = {\n",
    "            \"max_phenotype\": self.population.breeding_values.max().cpu().item(),\n",
    "            \"genetic_variance\": self.population.breeding_values.var().cpu().item(),\n",
    "            \"current_generation\": self.current_generation\n",
    "            }\n",
    "\n",
    "        return observation, reward, bool(terminated), False, info\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "class CustomFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 64):\n",
    "        super(CustomFeatureExtractor, self).__init__(observation_space, features_dim)\n",
    "        \n",
    "        # For now, a simple linear layer for the scalar input\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(observation_space.shape[0], features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        return self.linear(observations)\n",
    "\n",
    "class CustomPolicy(ActorCriticPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomPolicy, self).__init__(\n",
    "            *args,\n",
    "            **kwargs,\n",
    "            features_extractor_class=CustomFeatureExtractor,\n",
    "            features_extractor_kwargs=dict(features_dim=64)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "390e51dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2324, 0.2336, 0.0000, 1.2855, 0.4397, 0.2425, 0.0000, 0.7930, 0.0579,\n",
       "         0.0000, 0.7679, 0.0000, 0.7645, 0.0000, 0.8097, 0.3772, 0.4757, 0.0000,\n",
       "         0.0000, 0.0550, 0.7841, 0.0000, 0.0000, 0.0000, 0.7531, 0.2036, 0.1383,\n",
       "         0.0000, 0.3655, 0.2128, 0.0000, 0.0000, 0.2283, 0.0000, 0.0197, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.1436, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.2302, 0.0000, 0.0000, 0.0000, 0.0000, 0.4654, 0.9486, 0.0000, 0.6903,\n",
       "         1.1337, 0.1533, 0.5235, 0.0000, 0.5588, 0.8903, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FE = CustomFeatureExtractor(env.observation_space)\n",
    "mock_observation = th.tensor([[0.5]], dtype=th.float32)  # Single scalar input\n",
    "FE(mock_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f15ba6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.], dtype=float32),\n",
       " {'max_phenotype': 5.6667561531066895,\n",
       "  'genetic_variance': 1.000000238418579,\n",
       "  'current_generation': 0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = create_simulation()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7483d91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 11       |\n",
      "|    ep_rew_mean     | 106      |\n",
      "| time/              |          |\n",
      "|    fps             | 746      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f3899b2a6d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PPO(CustomPolicy, env, verbose=1)\n",
    "\n",
    "model.learn(total_timesteps=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aefe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_simulation(config=None):\n",
    "    if config is None:\n",
    "        config = get_default_config()\n",
    "    \n",
    "    seed = config['seed']\n",
    "    set_seed(seed)\n",
    "    \n",
    "    G = Genome(config['n_chr'], config['n_loci'], seed=seed)\n",
    "    \n",
    "    founder_haplotypes = np.load('../nbs/data/g2f_ch10.npy')\n",
    "    random_parent_indices = np.random.choice(founder_haplotypes.shape[0], config['n_parents'], replace=False)\n",
    "    random_loci_indices = np.random.choice(founder_haplotypes.shape[2], config['n_loci'], replace=False)\n",
    "    \n",
    "    founder_haplotypes = founder_haplotypes[random_parent_indices,:,:]\n",
    "    founder_haplotypes = founder_haplotypes[:,:,random_loci_indices]\n",
    "    founder_haplotypes = torch.tensor(founder_haplotypes).unsqueeze(2)\n",
    "    \n",
    "    inbred_founders = create_pop(G, founder_haplotypes)\n",
    "    f1 = create_pop(G, random_crosses(inbred_founders.haplotypes, 1000))\n",
    "    founder_pop = create_pop(G, random_crosses(f1.haplotypes, config['pop_size']))\n",
    "    \n",
    "    T = Trait(G, founder_pop, target_mean=config['target_mean'], target_variance=config['target_variance'], seed=seed)\n",
    "    \n",
    "    SP = SimParams(founder_pop, config)\n",
    "    env = SelectionIntensityEnvironment(SP, config)\n",
    "    \n",
    "    return env\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e309b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_default_config()\n",
    "config['n_parents'] = 2\n",
    "config['n_loci'] = 50\n",
    "config['pop_size'] = 100\n",
    "config['max_generations'] = 10\n",
    "config['total_timesteps'] = 5000000\n",
    "config['seed'] = 8\n",
    "for i in config:\n",
    "    print(f\"{i} : {config[i]}\")\n",
    "env = create_simulation(config)\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55bc5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def collect_baselines(env, actions, repetitions=10, cycles=5):\n",
    "    results = {action: {'max_phenotype': [], 'gv': []} for action in actions}\n",
    "    final_gen_averages = {}\n",
    "    env.reset()\n",
    "    for action in actions:\n",
    "        final_gen_phenotypes = []\n",
    "        for _ in range(repetitions):\n",
    "            env.reset()\n",
    "            cycle_max_phenotype = []\n",
    "            cycle_gv = []\n",
    "            cycle_max_phenotype.append(env.population.breeding_values.max())\n",
    "            cycle_gv.append(env.population.breeding_values.var())\n",
    "            for _ in range(cycles):\n",
    "                env.step(np.array([action]))\n",
    "                max_phenotype = env.population.breeding_values.max()\n",
    "                gv = env.population.breeding_values.var()\n",
    "                cycle_max_phenotype.append(max_phenotype)\n",
    "                cycle_gv.append(gv)\n",
    "            \n",
    "            results[action]['max_phenotype'].append(cycle_max_phenotype)\n",
    "            results[action]['gv'].append(cycle_gv)\n",
    "            final_gen_phenotypes.append(cycle_max_phenotype[-1])\n",
    "        \n",
    "        final_gen_averages[action] = np.mean(final_gen_phenotypes)\n",
    "    \n",
    "    best_action = max(final_gen_averages, key=final_gen_averages.get)\n",
    "    best_average = final_gen_averages[best_action]\n",
    "    \n",
    "    return results, best_action, best_average\n",
    "\n",
    "def plot_best_run(results, best_action):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    max_phenotypes = np.array(results[best_action]['max_phenotype'])\n",
    "    mean_values = np.mean(max_phenotypes, axis=0)\n",
    "    std_values = np.std(max_phenotypes, axis=0)\n",
    "    cycles = range(len(mean_values))\n",
    "    best_action_translated = scale_values(best_action)\n",
    "    \n",
    "    plt.plot(cycles, mean_values, label=f'Action {best_action_translated:.3f}')\n",
    "    plt.fill_between(cycles, mean_values - std_values, mean_values + std_values, alpha=0.3)\n",
    "    \n",
    "    plt.xlabel('Cycle')\n",
    "    plt.ylabel('Max Phenotype')\n",
    "    plt.title(f'Max Phenotype over Cycles for Best Action')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Usage\n",
    "actions = np.linspace(-1, 0, 10)  # or however many actions you want to test\n",
    "results, best_action, best_average = collect_baselines(env, actions, repetitions=10, cycles=env.SP.max_generations)\n",
    "\n",
    "print(f\"Best action: {best_action:.3f}\")\n",
    "print(f\"Best average max phenotype in final generation: {best_average:.3f}\")\n",
    "\n",
    "# Plot only the best run\n",
    "plot_best_run(results, best_action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3889e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    tensorboard_log='../dev_nbs/ppotb'\n",
    ")\n",
    "\n",
    "\n",
    "# model = PPO(CustomActorCriticPolicy, vec_env, verbose=1, tensorboard_log=\"./ppotb\")\n",
    "# model.learn(total_timesteps=config['total_timesteps'], callback=combined_callbacks)\n",
    "\n",
    "# Create your custom callbacks\n",
    "genetic_variance_callback = AverageFinalGenerationCallback(log_freq=1000)\n",
    "action_callback = ActionTrackingCallback(log_freq=1000)\n",
    "\n",
    "# Combine the callbacks using CallbackList\n",
    "combined_callbacks = CallbackList([genetic_variance_callback, action_callback])\n",
    "model.learn(total_timesteps=config['total_timesteps'], callback=combined_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca90bdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27e043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(env.population.breeding_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104713e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08104ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24253a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.SP.max_generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf4bf2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71b61ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050415eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create your custom callbacks\n",
    "genetic_variance_callback = AverageFinalGenerationCallback(log_freq=1000)\n",
    "action_callback = ActionTrackingCallback(log_freq=1000)\n",
    "\n",
    "# Combine the callbacks using CallbackList\n",
    "combined_callbacks = CallbackList([genetic_variance_callback, action_callback])\n",
    "vec_env = DummyVecEnv([lambda: env])\n",
    "# Create and train the model with the custom policy\n",
    "model = PPO(CustomActorCriticPolicy, vec_env, verbose=1, tensorboard_log=\"./ppotb\")\n",
    "model.learn(total_timesteps=config['total_timesteps'], callback=combined_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5ec2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['sparse_reward'] =True\n",
    "env = create_simulation(config)\n",
    "env.reset()\n",
    "# Create your custom callbacks\n",
    "genetic_variance_callback = AverageFinalGenerationCallback(log_freq=1000)\n",
    "action_callback = ActionTrackingCallback(log_freq=1000)\n",
    "\n",
    "# Combine the callbacks using CallbackList\n",
    "combined_callbacks = CallbackList([genetic_variance_callback, action_callback])\n",
    "vec_env = DummyVecEnv([lambda: env])\n",
    "# Create and train the model with the custom policy\n",
    "model = PPO(CustomActorCriticPolicy, vec_env, verbose=1, tensorboard_log=\"./ppotb\")\n",
    "model.learn(total_timesteps=config['total_timesteps'], callback=combined_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d60cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
