{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7570e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x01_populationStatistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9e0107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "import pdb\n",
    "import torch\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "device='cpu'\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "from chewc.config import *\n",
    "from chewc.sim import *\n",
    "from chewc.callback import *\n",
    "from chewc.policy import *\n",
    "from chewc.lab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "050415eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'population': array([[[[0, 0, 0, ..., 1, 0, 1]],\n",
       "  \n",
       "          [[1, 0, 0, ..., 1, 1, 1]]],\n",
       "  \n",
       "  \n",
       "         [[[1, 0, 0, ..., 0, 1, 0]],\n",
       "  \n",
       "          [[1, 1, 1, ..., 1, 0, 1]]],\n",
       "  \n",
       "  \n",
       "         [[[1, 0, 1, ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0, ..., 1, 0, 1]]],\n",
       "  \n",
       "  \n",
       "         ...,\n",
       "  \n",
       "  \n",
       "         [[[1, 0, 1, ..., 1, 0, 1]],\n",
       "  \n",
       "          [[0, 1, 0, ..., 0, 0, 0]]],\n",
       "  \n",
       "  \n",
       "         [[[1, 0, 1, ..., 1, 0, 1]],\n",
       "  \n",
       "          [[0, 1, 0, ..., 0, 0, 0]]],\n",
       "  \n",
       "  \n",
       "         [[[1, 0, 1, ..., 0, 0, 0]],\n",
       "  \n",
       "          [[1, 0, 0, ..., 1, 1, 1]]]], dtype=int32),\n",
       "  'generation': array([0.], dtype=float32)},\n",
       " {'max_phenotype': 1.8184278011322021,\n",
       "  'genetic_variance': 0.9999999403953552,\n",
       "  'current_generation': 0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = create_simulation()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c2255bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./ppotb/PPO_4\n",
      "----------------------------------\n",
      "| final_generation/   |          |\n",
      "|    genetic_variance | 0.351    |\n",
      "|    max_phenotype    | 6.48     |\n",
      "| time/               |          |\n",
      "|    fps              | 349      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| final_generation/       |              |\n",
      "|    genetic_variance     | 0.348        |\n",
      "|    max_phenotype        | 6.61         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060429648 |\n",
      "|    clip_fraction        | 0.0978       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.00967      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.34         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | 0.00114      |\n",
      "|    std                  | 0.993        |\n",
      "|    value_loss           | 28.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| final_generation/       |             |\n",
      "|    genetic_variance     | 0.359       |\n",
      "|    max_phenotype        | 6.52        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 180         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008681079 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.677       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.8        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0027     |\n",
      "|    std                  | 0.986       |\n",
      "|    value_loss           | 26.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f6b39121ad0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env = DummyVecEnv([lambda: env])\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Create your custom callbacks\n",
    "genetic_variance_callback = AverageFinalGenerationCallback(log_freq=100)\n",
    "action_callback = ActionTrackingCallback(log_freq=10)\n",
    "\n",
    "# Combine the callbacks using CallbackList\n",
    "combined_callbacks = CallbackList([genetic_variance_callback, action_callback])\n",
    "# Wrap the environment\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "\n",
    "# Create and train the model with the custom policy\n",
    "model = PPO(CustomActorCriticPolicy, env, verbose=1, tensorboard_log=\"./ppotb\")\n",
    "model.learn(total_timesteps=5000, callback=combined_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13d84abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chewc.lab.SelectionIntensityEnvironment at 0x7f6b428b3b50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2d977ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3963]])\n",
      "[2.9452908]\n",
      "tensor([[0.3521]])\n",
      "[3.2216918]\n",
      "tensor([[0.3587]])\n",
      "[3.7242708]\n",
      "tensor([[0.3814]])\n",
      "[4.430528]\n",
      "tensor([[0.4076]])\n",
      "[5.0033627]\n",
      "tensor([[0.4417]])\n",
      "[5.38952]\n",
      "tensor([[0.4239]])\n",
      "[5.4180665]\n",
      "tensor([[0.4555]])\n",
      "[5.5561166]\n",
      "tensor([[0.4615]])\n",
      "[5.937843]\n",
      "tensor([[0.4764]])\n",
      "[6.361653]\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have an environment 'env' properly wrapped in a VecEnv\n",
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(scale_values(action))\n",
    "    print(reward)\n",
    "    # Process the observation, reward, and info as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e2cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert torch tensor to numpy array if necessary\n",
    "phenotypes = env.population.breeding_values.numpy()\n",
    "\n",
    "# Calculate summary statistics\n",
    "mean_phenotype = np.mean(phenotypes)\n",
    "median_phenotype = np.median(phenotypes)\n",
    "std_phenotype = np.std(phenotypes)\n",
    "min_phenotype = np.min(phenotypes)\n",
    "max_phenotype = np.max(phenotypes)\n",
    "sum_phenotype = np.sum(phenotypes)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Summary Statistics:\")\n",
    "print(f\"Mean: {mean_phenotype:.2f}\")\n",
    "print(f\"Median: {median_phenotype:.2f}\")\n",
    "print(f\"Standard Deviation: {std_phenotype:.2f}\")\n",
    "print(f\"Minimum: {min_phenotype:.2f}\")\n",
    "print(f\"Maximum: {max_phenotype:.2f}\")\n",
    "print(f\"Sum: {sum_phenotype:.2f}\")\n",
    "\n",
    "# Create histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(phenotypes, bins=10, edgecolor='black')\n",
    "plt.title('Histogram of Phenotypes')\n",
    "plt.xlabel('Phenotype Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51202cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dfe431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming SimulatedEnv and Population classes are already defined as in your code\n",
    "\n",
    "def collect_baselines(env, actions, repetitions=100, cycles=5):\n",
    "    results = {action: {'max_phenotype': [], 'gv': []} for action in actions}\n",
    "    \n",
    "    for action in actions:\n",
    "        for _ in range(repetitions):\n",
    "            env.reset()\n",
    "            cycle_max_phenotype = []\n",
    "            cycle_gv = []\n",
    "            max_phenotype = env.population.breeding_values.max()\n",
    "            gv = env.population.breeding_values.var()\n",
    "            cycle_max_phenotype.append(max_phenotype)\n",
    "            cycle_gv.append(gv)\n",
    "            for _ in range(cycles):\n",
    "                env.step(np.array(action))\n",
    "                max_phenotype = env.population.breeding_values.max()\n",
    "                gv = env.population.breeding_values.var()\n",
    "                cycle_max_phenotype.append(max_phenotype)\n",
    "                cycle_gv.append(gv)\n",
    "            \n",
    "            results[action]['max_phenotype'].append(cycle_max_phenotype)\n",
    "            results[action]['gv'].append(cycle_gv)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_results(results, metric):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for action, data in results.items():\n",
    "        mean_values = np.mean(data[metric], axis=0)\n",
    "        std_values = np.std(data[metric], axis=0)\n",
    "        cycles = range(1, len(mean_values) + 1)\n",
    "        \n",
    "        plt.plot(cycles, mean_values, label=f'Action {action}')\n",
    "        plt.fill_between(cycles, mean_values - std_values, mean_values + std_values, alpha=0.3)\n",
    "    \n",
    "    plt.xlabel('Cycle')\n",
    "    plt.ylabel(metric.replace('_', ' ').title())\n",
    "    plt.title(f'{metric.replace(\"_\", \" \").title()} over Cycles')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Set up the environment and actions\n",
    "actions = [.05, .5, .95 ]\n",
    "actions = [scale_values(x, to_range=(-1, 1), from_range=(env.action_low, env.action_high)) for x in actions]\n",
    "print(actions)\n",
    "# Collect baselines\n",
    "results = collect_baselines(env, actions, repetitions=30,cycles=env.SP.max_generations)\n",
    "\n",
    "# Plot results\n",
    "plot_results(results, 'max_phenotype')\n",
    "plot_results(results, 'gv')\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba53a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad71eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Create the environment\n",
    "# config = {}\n",
    "# env = SelectionIntensityEnvironment(SP, config)\n",
    "\n",
    "# # Wrap the environment (required for SB3)\n",
    "# env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# # Create and train the model with the custom policy\n",
    "# model = PPO(CustomActorCriticPolicy, env, verbose=1, tensorboard_log=\"./ppotb\")\n",
    "# model.learn(total_timesteps=100000, callback=combined_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194269ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# data = rewards\n",
    "# # Calculate means and standard deviations\n",
    "# means = [np.mean(values) for values in data.values()]\n",
    "# stds = [np.std(values) for values in data.values()]\n",
    "\n",
    "# # Create x-axis values (1 to 15)\n",
    "# x = list(data.keys())\n",
    "\n",
    "# # Create the plot\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.errorbar(x, means, yerr=stds, fmt='o-', capsize=5, capthick=2, ecolor='gray')\n",
    "\n",
    "# # Customize the plot\n",
    "# plt.title('Average Values with Standard Deviation')\n",
    "# plt.xlabel('Step')\n",
    "# plt.ylabel('Value')\n",
    "# plt.xticks(x)  # Set x-axis ticks to match the step numbers\n",
    "# plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# # Add a legend\n",
    "# plt.legend(['Mean', 'Standard Deviation'])\n",
    "\n",
    "# # Show the plot\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c751c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# data = gvs\n",
    "# # Calculate means and standard deviations\n",
    "# means = [np.mean(values) for values in data.values()]\n",
    "# stds = [np.std(values) for values in data.values()]\n",
    "\n",
    "# # Create x-axis values (1 to 15)\n",
    "# x = list(data.keys())\n",
    "\n",
    "# # Create the plot\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.errorbar(x, means, yerr=stds, fmt='o-', capsize=5, capthick=2, ecolor='gray')\n",
    "\n",
    "# # Customize the plot\n",
    "# plt.title('Average Values with Standard Deviation')\n",
    "# plt.xlabel('Step')\n",
    "# plt.ylabel('Value')\n",
    "# plt.xticks(x)  # Set x-axis ticks to match the step numbers\n",
    "# plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# # Add a legend\n",
    "# plt.legend(['Mean', 'Standard Deviation'])\n",
    "\n",
    "# # Show the plot\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c6f5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# data = scaled_values\n",
    "# # Calculate means and standard deviations\n",
    "# means = [np.mean(values) for values in data.values()]\n",
    "# stds = [np.std(values) for values in data.values()]\n",
    "\n",
    "# # Create x-axis values (1 to 15)\n",
    "# x = list(data.keys())\n",
    "\n",
    "# # Create the plot\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.errorbar(x, means, yerr=stds, fmt='o-', capsize=5, capthick=2, ecolor='gray')\n",
    "\n",
    "# # Customize the plot\n",
    "# plt.title('Average Values with Standard Deviation')\n",
    "# plt.xlabel('Step')\n",
    "# plt.ylabel('Value')\n",
    "# plt.xticks(x)  # Set x-axis ticks to match the step numbers\n",
    "# plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# # Add a legend\n",
    "# plt.legend(['Mean', 'Standard Deviation'])\n",
    "\n",
    "# # Show the plot\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7010ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewards = {}\n",
    "# scaled_values = {}\n",
    "# gvs = {}\n",
    "\n",
    "# for _ in range(3):\n",
    "#     obs = env.reset()\n",
    "#     done = False\n",
    "#     while not done:\n",
    "#         action, _states = model.predict(obs, deterministic=True)\n",
    "#         obs, reward, done, info = env.step(action)\n",
    "\n",
    "#         scaled_value = scale_values(action)\n",
    "#         gen = env.buf_infos[0]['current_generation']\n",
    "        \n",
    "#         if gen not in rewards:\n",
    "#             rewards[gen] = []\n",
    "#         rewards[gen].append(float(env.buf_infos[0]['max_phenotype']))\n",
    "        \n",
    "#         if gen not in scaled_values:\n",
    "#             scaled_values[gen] = []\n",
    "#         scaled_values[gen].append(float(scaled_value))\n",
    "        \n",
    "#         if gen not in gvs:\n",
    "#             gvs[gen] = []\n",
    "#         gvs[gen].append(float(env.buf_infos[0]['genetic_variance']))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d05b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e4fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# # Wrap the environment (required for SB3)\n",
    "# # Create the environment\n",
    "# config = {'sparse_reward':True}\n",
    "# env = SelectionIntensityEnvironment(SP, config)\n",
    "# x = env.reset()\n",
    "\n",
    "# env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# # Create and train the model with the custom policy\n",
    "# model = PPO(CustomActorCriticPolicy, env, verbose=1, tensorboard_log=\"./ppotb\")\n",
    "# model.learn(total_timesteps=50000, callback=combined_callbacks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb75db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# # Wrap the environment (required for SB3)\n",
    "# # Create the environment\n",
    "# config = {'sparse_reward':False}\n",
    "# env = SelectionIntensityEnvironment(SP, config)\n",
    "# x = env.reset()\n",
    "\n",
    "# env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# # Create and train the model with the custom policy\n",
    "# model = PPO(CustomActorCriticPolicy, env, verbose=1, tensorboard_log=\"./ppotb\")\n",
    "# model.learn(total_timesteps=50000, callback=combined_callbacks)\n",
    "\n",
    "# # Assuming you have an environment 'env'\n",
    "# obs = env.reset()\n",
    "# done = False\n",
    "\n",
    "# while not done:\n",
    "#     action, _states = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, done, info = env.step(action)\n",
    "#     print(scale_values(action))\n",
    "#     print(reward)\n",
    "#     # Process the observation, reward, and info as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
