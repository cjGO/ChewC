{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f9c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5622ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import numpy as np\n",
    "from chewc.sim import *\n",
    "import torch\n",
    "\n",
    "class SelectionIntensityEnvironment(gym.Env):\n",
    "    def __init__(self, SP, config):\n",
    "        super(SelectionIntensityEnvironment, self).__init__()\n",
    "        self.SP = SP\n",
    "        self.config = config  # Store the config\n",
    "        self.current_generation = 0\n",
    "        self.max_generations = SP.max_generations\n",
    "        # Get action space bounds from config, with defaults if not provided\n",
    "        \n",
    "        self.action_low = config.get('action_low', 0.05)\n",
    "        self.action_high = config.get('action_high', 0.95)\n",
    "        \n",
    "        # Update action space with custom bounds\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=np.array([-1]), \n",
    "            high=np.array([1]), \n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "            \"population\": gym.spaces.Box(low=0, high=1, shape=(self.SP.pop_size, 2, self.SP.G.n_chr, self.SP.G.n_loci), dtype=np.int32),\n",
    "            \"generation\": gym.spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n",
    "        })\n",
    "        # logging\n",
    "        self.action_values = []\n",
    "        self.genetic_variance = []\n",
    "        self.max_breeding_values = []\n",
    "        self.final_generations = []\n",
    "        self.episode_count = 0\n",
    "        self.rewards = []\n",
    "        self.episode_reward = 0\n",
    "        \n",
    "        #config\n",
    "        self.config =config\n",
    "        \n",
    "    def _get_obs(self):\n",
    "        population = self.population.haplotypes.cpu().numpy().astype(np.int32)\n",
    "        generation = np.array([self.current_generation / self.SP.max_generations], dtype=np.float32)\n",
    "        return {\"population\": population, \"generation\": generation}\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"max_phenotype\": self.population.breeding_values.max().cpu().item(),\n",
    "            \"genetic_variance\": self.population.breeding_values.var().cpu().item(),\n",
    "            \"current_generation\": self.current_generation\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.population = self.SP.founder_pop\n",
    "        self.phenotype = phenotype(self.population, self.SP.T, self.SP.h2)\n",
    "        self.current_generation = 0\n",
    "        self.episode_reward = 0\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        # Map the action from [-1, 1] to [action_low, action_high]\n",
    "        action = scale_values(action, to_range=(self.action_low, self.action_high))\n",
    "#         print(action)\n",
    "\n",
    "        # Ensure action_scalar is within bounds\n",
    "        total_selected = max((2,int(action * self.population.size)))\n",
    "#         print(f'total selected {total_selected}')\n",
    "#         action_scalar = np.clip(action_scalar, self.action_low, self.action_high)\n",
    "        selected = torch.topk(self.population.phenotypes, total_selected).indices\n",
    "        self.population = create_pop(self.SP.G, random_crosses(self.population.haplotypes[selected], self.SP.pop_size))\n",
    "        self.phenotype = phenotype(self.population, self.SP.T, self.SP.h2)\n",
    "        self.current_generation += 1\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        info['normalized_action'] = action\n",
    "\n",
    "\n",
    "        \n",
    "        terminated = self.current_generation >= self.SP.max_generations\n",
    "        #REWARD\n",
    "        if self.config.get('sparse_reward', False):  # Use .get() with a default value\n",
    "            reward = 0 if not terminated else float(self.population.breeding_values.max())\n",
    "        else:\n",
    "            reward = float(self.population.breeding_values.max())\n",
    "        self.episode_reward += reward\n",
    "\n",
    "        if terminated:\n",
    "            info['final_generation'] = {\n",
    "            \"max_phenotype\": self.population.breeding_values.max().cpu().item(),\n",
    "            \"genetic_variance\": self.population.breeding_values.var().cpu().item(),\n",
    "            \"current_generation\": self.current_generation\n",
    "            }\n",
    "\n",
    "        return observation, reward, bool(terminated), False, info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc070b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80ae4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
