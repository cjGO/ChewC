{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7570e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x01_populationStatistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9e0107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "import pdb\n",
    "import torch\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "device='cpu'\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "### BREEDING SIMULATOR\n",
    "class Genome:\n",
    "    def __init__(self, n_chr, n_loci, seed=None):\n",
    "        self.ploidy = 2\n",
    "        self.n_chr = n_chr\n",
    "        self.n_loci = n_loci\n",
    "        self.shape = (self.ploidy, self.n_chr, self.n_loci)\n",
    "        if seed is not None:\n",
    "            set_seed(seed)\n",
    "\n",
    "class Population:\n",
    "    def __init__(self, genome, haplotypes, device=device):\n",
    "        self.genome = genome\n",
    "        self.device = device\n",
    "        self.phenotypes = None\n",
    "        self.bvs = None\n",
    "        self.haplotypes = haplotypes.to(device)\n",
    "        self.dosages = haplotypes.sum(dim=1).float().to(device)\n",
    "        self.size = haplotypes.shape[0]\n",
    "\n",
    "class Trait:\n",
    "    def __init__(self, genome, founder_population, target_mean, target_variance, device=device, seed=None):\n",
    "        if seed is not None:\n",
    "            set_seed(seed)\n",
    "        self.target_mean = target_mean\n",
    "        self.target_variance = target_variance\n",
    "        self.device = device\n",
    "        random_effects = torch.randn(genome.n_chr, genome.n_loci, device=self.device)\n",
    "\n",
    "        self.device = device\n",
    "        random_effects = torch.randn(genome.n_chr, genome.n_loci, device=self.device)\n",
    "        random_effects -= random_effects.mean()\n",
    "        founder_scores = torch.einsum('kl,hkl->h', random_effects, founder_population.dosages).to(device)\n",
    "        founder_mean, founder_var = founder_scores.mean(), founder_scores.var()\n",
    "        scaling_factors = torch.sqrt(self.target_variance / founder_var)\n",
    "        self.scaling_factors = scaling_factors\n",
    "        random_effects *= scaling_factors\n",
    "        self.effects = random_effects\n",
    "        self.intercept = founder_mean - target_mean\n",
    "\n",
    "\n",
    "def calculate_breeding_value(population, trait, device=device):\n",
    "    return torch.einsum('hjk,jk->h', population.dosages, trait.effects).to(device)\n",
    "\n",
    "def truncation_selection(population, trait, top_percent):\n",
    "    return torch.topk(population.phenotypes, top_percent).indices.to(device)\n",
    "\n",
    "# meiosis\n",
    "def recombine(parent_haplo_tensor, recombination_rate=0.1):\n",
    "    num_individuals, ploidy, num_chromosomes, num_loci = parent_haplo_tensor.shape\n",
    "    # Generate crossover masks\n",
    "    maternal, paternal = parent_haplo_tensor[:,0,:,:], parent_haplo_tensor[:,1,:,:]\n",
    "    crossovers = torch.bernoulli(torch.full((num_individuals, num_chromosomes, num_loci), recombination_rate, device=device))\n",
    "    progeny = maternal * (1 - crossovers) + paternal * crossovers\n",
    "    return progeny\n",
    "\n",
    "\n",
    "\n",
    "def truncation_selection(population, trait, top_percent):\n",
    "    return torch.topk(population.phenotypes, top_percent).indices.to(device)\n",
    "\n",
    "# meiosis\n",
    "def recombine(parent_haplo_tensor, recombination_rate=0.1):\n",
    "    num_individuals, ploidy, num_chromosomes, num_loci = parent_haplo_tensor.shape\n",
    "    # Generate crossover masks\n",
    "    maternal, paternal = parent_haplo_tensor[:,0,:,:], parent_haplo_tensor[:,1,:,:]\n",
    "    crossovers = torch.bernoulli(torch.full((num_individuals, num_chromosomes, num_loci), recombination_rate, device=device))\n",
    "    progeny = maternal * (1 - crossovers) + paternal * crossovers\n",
    "    return progeny\n",
    "\n",
    "def phenotype(population, trait, h2):\n",
    "    breeding_values = calculate_breeding_value(population, trait)\n",
    "    population.breeding_values = breeding_values\n",
    "    if breeding_values.var() == 0:\n",
    "        environmental_variance = 0\n",
    "    else:\n",
    "        environmental_variance = (1 - h2) / h2 * breeding_values.var()\n",
    "\n",
    "    # Check if environmental_variance is zero before applying torch.sqrt and .clone()\n",
    "    if environmental_variance == 0:\n",
    "        environmental_noise = torch.zeros(breeding_values.shape, device=device)\n",
    "    else:\n",
    "        environmental_noise = torch.randn(breeding_values.shape, device=device) * torch.sqrt(environmental_variance).detach()\n",
    "\n",
    "    population.breeding_values = breeding_values\n",
    "    population.phenotypes = breeding_values + environmental_noise\n",
    "\n",
    "    return population.phenotypes.max()\n",
    "\n",
    "def create_random_pop(G, pop_size):\n",
    "    return torch.randint(0, 2, (pop_size, *G.shape), device=device)\n",
    "\n",
    "def update_pop(population, haplotype_pop_tensor):\n",
    "    population.haplotypes = haplotype_pop_tensor\n",
    "    population.dosages = haplotype_pop_tensor.sum(dim=1).float()\n",
    "    return population\n",
    "\n",
    "# meiosis\n",
    "def recombine(parent_haplo_tensor, recombination_rate=0.1):\n",
    "    num_individuals, ploidy, num_chromosomes, num_loci = parent_haplo_tensor.shape\n",
    "    # Generate crossover masks\n",
    "    maternal, paternal = parent_haplo_tensor[:,0,:,:],parent_haplo_tensor[:,1,:,:],\n",
    "    crossovers = torch.bernoulli(torch.full((num_individuals, num_chromosomes, num_loci), recombination_rate, device=device))\n",
    "#     crossovers = torch.rand((num_individuals, num_chromosomes, num_loci), device=device) < recombination_rate\n",
    "    progeny = maternal * torch.logical_not(crossovers) + paternal * crossovers\n",
    "    return progeny\n",
    "\n",
    "def breed(mother_tensor, father_tensor, recombination_rate=0.1):\n",
    "    eggs = recombine(mother_tensor,recombination_rate)\n",
    "    pollens = recombine(father_tensor,recombination_rate)\n",
    "    return torch.stack((eggs,pollens), dim=1)\n",
    "\n",
    "def create_pop(G, haplotypes):\n",
    "    return Population(G, haplotypes=haplotypes)\n",
    "\n",
    "def bv(P,T):\n",
    "    P.breeding_values = calculate_breeding_value(P.dosages,T.effects)\n",
    "\n",
    "def create_progeny(mother_gametes, father_gametes, reps=1, device=device):\n",
    "    progeny = []\n",
    "    for _ in range(reps):\n",
    "        # Randomly shuffle the gametes from each parent\n",
    "        shuffled_mother_indices = torch.randperm(mother_gametes.shape[0], device=device)\n",
    "        shuffled_father_indices = torch.randperm(father_gametes.shape[0], device=device)\n",
    "\n",
    "        # Select the shuffled gametes\n",
    "        mother_gametes = mother_gametes[shuffled_mother_indices]\n",
    "        father_gametes = father_gametes[shuffled_father_indices]\n",
    "\n",
    "        # Stack the gametes to create progeny haplotypes\n",
    "        progeny_haplotypes = torch.stack((mother_gametes, father_gametes), dim=1)\n",
    "        progeny.append(progeny_haplotypes)\n",
    "    return torch.vstack(progeny)\n",
    "\n",
    "\n",
    "def random_crosses(parent_population, total_crosses, device=device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    num_parents = parent_population.shape[0]\n",
    "    ploidy, num_chromosomes, num_loci = parent_population.shape[1:]\n",
    "    \n",
    "    # Randomly select parents for each cross\n",
    "    parent_indices = torch.randint(0, num_parents, (total_crosses, 2), device=device)\n",
    "    \n",
    "    # Select the parent haplotypes\n",
    "    mothers = parent_population[parent_indices[:, 0]]\n",
    "    fathers = parent_population[parent_indices[:, 1]]\n",
    "    \n",
    "    # Perform recombination for both parents\n",
    "    mother_gametes = recombine(mothers)\n",
    "    father_gametes = recombine(fathers)\n",
    "    \n",
    "    # Combine gametes to create progeny\n",
    "    progeny = torch.stack((mother_gametes, father_gametes), dim=1)\n",
    "    \n",
    "    return progeny\n",
    "\n",
    "def phenotype(population, trait, h2):\n",
    "    breeding_values = calculate_breeding_value(population, trait)\n",
    "    population.breeding_values = breeding_values\n",
    "    population.genetic_var = breeding_values.var()\n",
    "    if breeding_values.var() == 0:\n",
    "        environmental_variance = 0\n",
    "    else:\n",
    "        environmental_variance = (1 - h2) / h2 * breeding_values.var()\n",
    "\n",
    "    # Check if environmental_variance is zero before applying torch.sqrt and .clone()\n",
    "    if environmental_variance == 0:\n",
    "        environmental_noise = torch.zeros(breeding_values.shape, device=device)\n",
    "    else:\n",
    "        environmental_noise = torch.randn(breeding_values.shape, device=device) * torch.sqrt(environmental_variance).detach()\n",
    "    population.phenotypes = breeding_values + environmental_noise\n",
    "\n",
    "class SimParams:\n",
    "    def __init__(self, G, T, h2, reps, pop_size, max_generations, founder_pop, seed=None):\n",
    "        self.G = G\n",
    "        self.T = T\n",
    "        self.h2 = h2\n",
    "        self.reps = reps\n",
    "        self.pop_size = pop_size\n",
    "        self.max_generations = max_generations\n",
    "        self.founder_pop = founder_pop\n",
    "        self.seed = seed\n",
    "        if seed is not None:\n",
    "            set_seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf7ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bb0a3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from typing import Tuple, Callable\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class CustomFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Dict):\n",
    "        super().__init__(observation_space, features_dim=64)\n",
    "        \n",
    "        pop_shape = observation_space.spaces[\"population\"].shape\n",
    "        self.pop_extractor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(pop_shape[0] * pop_shape[1] * pop_shape[2] * pop_shape[3], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.gen_extractor = nn.Sequential(\n",
    "            nn.Linear(1, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.combined = nn.Sequential(\n",
    "            nn.Linear(80, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, observations) -> th.Tensor:\n",
    "        pop_features = self.pop_extractor(observations[\"population\"])\n",
    "        gen_features = self.gen_extractor(observations[\"generation\"])\n",
    "        combined = th.cat([pop_features, gen_features], dim=1)\n",
    "        return self.combined(combined)\n",
    "\n",
    "class CustomNetwork(nn.Module):\n",
    "    def __init__(self, feature_dim: int, last_layer_dim_pi: int = 64, last_layer_dim_vf: int = 64):\n",
    "        super().__init__()\n",
    "        self.latent_dim_pi = last_layer_dim_pi\n",
    "        self.latent_dim_vf = last_layer_dim_vf\n",
    "        \n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, last_layer_dim_pi),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, last_layer_dim_vf),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        return self.forward_actor(features), self.forward_critic(features)\n",
    "\n",
    "    def forward_actor(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.policy_net(features)\n",
    "\n",
    "    def forward_critic(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.value_net(features)\n",
    "\n",
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        lr_schedule: Callable[[float], float],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        kwargs[\"features_extractor_class\"] = CustomFeatureExtractor\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        self.mlp_extractor = CustomNetwork(self.features_dim)\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Assuming `SelectionIntensityEnvironment` is defined as per your provided context\n",
    "class SelectionIntensityEnvironment(gym.Env):\n",
    "    def __init__(self, SP, config):\n",
    "        super(SelectionIntensityEnvironment, self).__init__()\n",
    "        self.SP = SP\n",
    "        self.config = config  # Store the config\n",
    "        self.current_generation = 0\n",
    "        self.max_generations = SP.max_generations\n",
    "        self.action_space = gym.spaces.Box(low=np.array([-1.0]), high=np.array([1.0]), dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "            \"population\": gym.spaces.Box(low=0, high=1, shape=(self.SP.pop_size, 2, self.SP.G.n_chr, self.SP.G.n_loci), dtype=np.int32),\n",
    "            \"generation\": gym.spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n",
    "        })\n",
    "        \n",
    "        # logging\n",
    "        self.action_values = []\n",
    "        self.genetic_variance = []\n",
    "        self.max_breeding_values = []\n",
    "        self.final_generations = []\n",
    "        self.episode_count = 0\n",
    "        self.rewards = []\n",
    "        self.episode_reward = 0\n",
    "        \n",
    "        #config\n",
    "        self.config =config\n",
    "    def _get_obs(self):\n",
    "        population = self.population.haplotypes.cpu().numpy().astype(np.int32)\n",
    "        generation = np.array([self.current_generation / self.SP.max_generations], dtype=np.float32)\n",
    "        return {\"population\": population, \"generation\": generation}\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"max_phenotype\": self.population.phenotypes.max().cpu().item(),\n",
    "            \"genetic_variance\": self.population.breeding_values.var().cpu().item(),\n",
    "            \"current_generation\": self.current_generation\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.population = self.SP.founder_pop\n",
    "        self.phenotype = phenotype(self.population, self.SP.T, self.SP.h2)\n",
    "        self.current_generation = 0\n",
    "        self.episode_reward = 0\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "#         action_scalar = action.item() if isinstance(action, np.ndarray) else action\n",
    "        action_scalar = 0.1 + 0.9 * (action.item() + 1) / 2\n",
    "        selected = torch.topk(self.population.phenotypes, int(action_scalar * self.population.size)).indices\n",
    "        self.population = create_pop(self.SP.G, random_crosses(self.population.haplotypes[selected], self.SP.pop_size))\n",
    "        self.phenotype = phenotype(self.population, self.SP.T, self.SP.h2)\n",
    "        self.current_generation += 1\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        info['normalized_action'] = action_scalar\n",
    "\n",
    "\n",
    "        \n",
    "        terminated = self.current_generation >= self.SP.max_generations or self.population.genetic_var < .005\n",
    "        #REWARD\n",
    "        if self.config.get('sparse_reward', False):  # Use .get() with a default value\n",
    "            reward = 0 if not terminated else float(self.population.breeding_values.max())\n",
    "        else:\n",
    "            reward = float(self.population.breeding_values.max())\n",
    "        self.episode_reward += reward\n",
    "\n",
    "        if terminated:\n",
    "            info['final_generation'] = {\n",
    "            \"max_phenotype\": self.population.phenotypes.max().cpu().item(),\n",
    "            \"genetic_variance\": self.population.breeding_values.var().cpu().item(),\n",
    "            \"current_generation\": self.current_generation\n",
    "            }\n",
    "\n",
    "        return observation, reward, bool(terminated), False, info\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from collections import defaultdict\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from collections import defaultdict\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class ActionTrackingCallback(BaseCallback):\n",
    "    def __init__(self, log_freq=100, verbose=0):\n",
    "        super(ActionTrackingCallback, self).__init__(verbose)\n",
    "        self.log_freq = log_freq\n",
    "        self.episode_count = 0\n",
    "        self.action_history = defaultdict(list)\n",
    "        self.current_episode_actions = []\n",
    "        self.max_generations = None\n",
    "        self.step_count = 0\n",
    "        self.writer = None\n",
    "\n",
    "    def _on_training_start(self):\n",
    "        self.writer = SummaryWriter(self.logger.dir)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        self.step_count += 1\n",
    "        info = self.locals['infos'][0]\n",
    "        action = self.locals['actions'][0]\n",
    "        current_generation = info['current_generation']\n",
    "        \n",
    "        if self.max_generations is None:\n",
    "            self.max_generations = self.training_env.get_attr('max_generations')[0]\n",
    "\n",
    "        # Normalize action from [-1, 1] to [0, 1]\n",
    "#         normalized_action = (action.item() + 1) / 2\n",
    "    # Use the normalized action from the environment\n",
    "        normalized_action = info['normalized_action']\n",
    "\n",
    "        self.current_episode_actions.append((current_generation, normalized_action))\n",
    "\n",
    "        if self.locals['dones'][0]:\n",
    "            self.episode_count += 1\n",
    "            for gen, action in self.current_episode_actions:\n",
    "                self.action_history[gen].append(action)\n",
    "            self.current_episode_actions = []\n",
    "\n",
    "            if self.episode_count % self.log_freq == 0:\n",
    "                self._log_action_data()\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _log_action_data(self):\n",
    "        action_data = {}\n",
    "        for gen in range(self.max_generations):\n",
    "            if gen in self.action_history:\n",
    "                avg_action = np.mean(self.action_history[gen])\n",
    "                action_data[f\"generation_{gen}\"] = avg_action\n",
    "\n",
    "        # Log all generation data at once\n",
    "        self.writer.add_scalars(\"average_actions\", action_data, self.step_count)\n",
    "\n",
    "        # Clear the action history after logging\n",
    "        self.action_history.clear()\n",
    "\n",
    "    def on_training_end(self):\n",
    "        # Final logging of action data\n",
    "        self._log_action_data()\n",
    "        if self.writer:\n",
    "            self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4030499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from collections import defaultdict\n",
    "class AverageFinalGenerationCallback(BaseCallback):\n",
    "    def __init__(self, log_freq=100, verbose=0):\n",
    "        super(AverageFinalGenerationCallback, self).__init__(verbose)\n",
    "        self.log_freq = log_freq\n",
    "        self.phenotypes = []\n",
    "        self.genetic_variances = []\n",
    "        self.episode_count = 0\n",
    "        self.all_action_values = []  # Store all action values\n",
    "        self.current_episode_actions = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        action = self.locals['actions'][0]\n",
    "        self.current_episode_actions.append(action.item())\n",
    "        \n",
    "        for env_idx, done in enumerate(self.locals['dones']):\n",
    "            if done:\n",
    "                info = self.locals['infos'][env_idx]\n",
    "                if 'final_generation' in info:\n",
    "                    self.episode_count += 1\n",
    "                    final_gen_info = info['final_generation']\n",
    "                    \n",
    "                    phenotype = final_gen_info['max_phenotype']\n",
    "                    if isinstance(phenotype, torch.Tensor):\n",
    "                        phenotype = phenotype.cpu().numpy()\n",
    "                    self.phenotypes.append(phenotype)\n",
    "                    \n",
    "                    genetic_variance = final_gen_info['genetic_variance']\n",
    "                    if isinstance(genetic_variance, torch.Tensor):\n",
    "                        genetic_variance = genetic_variance.cpu().numpy()\n",
    "                    self.genetic_variances.append(genetic_variance)\n",
    "                    \n",
    "                    # Store actions for this episode\n",
    "                    self.all_action_values.extend(self.current_episode_actions)\n",
    "                    self.current_episode_actions = []  # Reset for next episode\n",
    "\n",
    "                    # Log every log_freq episodes\n",
    "                    if self.episode_count % self.log_freq == 0:\n",
    "                        self._log_data()\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _log_data(self):\n",
    "        avg_phenotype = np.mean(self.phenotypes)\n",
    "        avg_genetic_variance = np.mean(self.genetic_variances)\n",
    "        self.logger.record(\"final_generation/max_phenotype\", avg_phenotype)\n",
    "        self.logger.record(\"final_generation/genetic_variance\", avg_genetic_variance)\n",
    "        \n",
    "        # Plot action values\n",
    "        self._plot_action_values()\n",
    "        \n",
    "        # Reset lists for next logging period\n",
    "        self.phenotypes = []\n",
    "        self.genetic_variances = []\n",
    "        self.all_action_values = []  # Store all action values\n",
    "\n",
    "    def _plot_action_values(self):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self.all_action_values)\n",
    "        plt.title(\"Action Values Across All Training Steps\")\n",
    "        plt.xlabel(\"Training Step\")\n",
    "        plt.ylabel(\"Action Value\")\n",
    "        plt.ylim(0, 1)  # Assuming action values are between 0 and 1\n",
    "        plt.savefig(f\"action_values_plot_{self.episode_count}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def on_training_end(self):\n",
    "        # Plot final action values chart\n",
    "        self._plot_action_values()\n",
    "import numpy as np\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe1c121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import random\n",
    "\n",
    "\n",
    "def create_random_pop(G, pop_size, seed=None):\n",
    "    if seed is not None:\n",
    "        set_seed(seed)\n",
    "    return torch.randint(0, 2, (pop_size, *G.shape), device=device)\n",
    "\n",
    "# Example usage\n",
    "n_chr = 1\n",
    "n_loci = 50\n",
    "founder_pop_size = 50\n",
    "h2 = 1\n",
    "reps = 1\n",
    "max_generations = 10\n",
    "seed = 42  # Choose any integer for your seed\n",
    "G = Genome(n_chr, n_loci, seed=seed)\n",
    "founder_pop = create_random_pop(G, 5, seed=seed)\n",
    "founder_pop = random_crosses(founder_pop, founder_pop_size, seed=seed)\n",
    "founder_pop = create_pop(G, founder_pop)\n",
    "T = Trait(G, founder_pop, target_mean=0.0, target_variance=1.0, seed=seed)\n",
    "SP = SimParams(G, T, h2, reps, founder_pop_size, max_generations, founder_pop, seed=seed)\n",
    "\n",
    "\n",
    "# Create your custom callbacks\n",
    "genetic_variance_callback = AverageFinalGenerationCallback(log_freq=100)\n",
    "action_callback = ActionTrackingCallback(log_freq=10)\n",
    "\n",
    "# Combine the callbacks using CallbackList\n",
    "combined_callbacks = CallbackList([genetic_variance_callback, action_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78dfe431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5068,  0.4911,  0.1967, -0.3860, -0.1819,  0.1038, -0.3180,  0.0928,\n",
      "         -0.0447,  0.0126,  0.2412,  0.0702,  0.0215, -0.4001,  0.2310,  0.5647,\n",
      "         -0.2091,  0.0209,  0.1930,  0.3839,  0.1495,  0.0038, -0.7840, -0.0875,\n",
      "          0.3353, -0.3942, -0.0969,  0.2762,  0.5606,  0.2179, -0.6181,  0.0872,\n",
      "         -0.5624, -0.1408,  0.0173,  0.1030,  0.5011,  0.0279, -0.6494, -0.6064,\n",
      "         -0.1807, -0.1589,  0.3438,  0.6023, -0.1248,  0.0640,  0.2089, -0.2299,\n",
      "         -0.4098, -0.0452]])\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "n_chr = 1\n",
    "n_loci = 50\n",
    "founder_pop_size = 50\n",
    "h2 = 1\n",
    "reps = 1\n",
    "max_generations = 10\n",
    "seed = 40  # Choose any integer for your seed\n",
    "G = Genome(n_chr, n_loci, seed=seed)\n",
    "founder_pop = create_random_pop(G, 5, seed=seed)\n",
    "founder_pop = random_crosses(founder_pop, founder_pop_size, seed=seed)\n",
    "founder_pop = create_pop(G, founder_pop)\n",
    "T = Trait(G, founder_pop, target_mean=0.0, target_variance=1.0, seed=seed)\n",
    "SP = SimParams(G, T, h2, reps, founder_pop_size, max_generations, founder_pop, seed=seed)\n",
    "print(T.effects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbb75db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./ppotb/PPO_72\n",
      "----------------------------------\n",
      "| final_generation/   |          |\n",
      "|    genetic_variance | 0.314    |\n",
      "|    max_phenotype    | 6.46     |\n",
      "| time/               |          |\n",
      "|    fps              | 680      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f89248d0b90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create your custom callbacks\n",
    "genetic_variance_callback = AverageFinalGenerationCallback(log_freq=100)\n",
    "action_callback = ActionTrackingCallback(log_freq=10)\n",
    "\n",
    "# Combine the callbacks using CallbackList\n",
    "combined_callbacks = CallbackList([genetic_variance_callback, action_callback])\n",
    "\n",
    "# Create the environment\n",
    "config = {'sparse_reward':True}\n",
    "env = SelectionIntensityEnvironment(SP, config)\n",
    "\n",
    "# Wrap the environment (required for SB3)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Create and train the model with the custom policy\n",
    "model = PPO(CustomActorCriticPolicy, env, verbose=1, tensorboard_log=\"./ppotb\")\n",
    "model.learn(total_timesteps=200, callback=combined_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad71eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your custom callbacks\n",
    "genetic_variance_callback = AverageFinalGenerationCallback(log_freq=100)\n",
    "action_callback = ActionTrackingCallback(log_freq=10)\n",
    "\n",
    "# Combine the callbacks using CallbackList\n",
    "combined_callbacks = CallbackList([genetic_variance_callback, action_callback])\n",
    "\n",
    "# Create the environment\n",
    "config = {}\n",
    "env = SelectionIntensityEnvironment(SP, config)\n",
    "\n",
    "# Wrap the environment (required for SB3)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Create and train the model with the custom policy\n",
    "model = PPO(CustomActorCriticPolicy, env, verbose=1, tensorboard_log=\"./ppotb\")\n",
    "model.learn(total_timesteps=200000, callback=combined_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0875922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
